train = pd.read_csv("SOC_Train_Non-Saliency.csv", header=None)

for i in range(200):

  train = train.sample(frac=1)  # 打乱顺序
  rate = 0.00001
  whole_loss = 0.0

  imgs = np.ndarray((18, 224, 224, 3), dtype=np.float32)
  labels = np.ndarray((18, 1), dtype=np.float32)

  for j, row in enumerate(train.iterrows()):
    if (j > 0) & (j % 18 == 0):
      train_imgs = np.reshape(imgs, (-1, 224, 224, 3))
      train_labels = np.reshape(labels, (-1))
      _, loss = sess.run([train_step, cross_entropy], feed_dict={xs: train_imgs, ys: train_labels, lr: rate})
      whole_loss += loss
      img = cv2.imread('H:/data/SOC/TrainSet/Imgs/' + row[1][0] + '.jpg').astype(np.float32)
      img = cv2.resize(img, (224, 224))
      img = img - vgg_mean
      imgs[j % 18] = img
      labels[j % 18] = row[1][1]

      print("Epoch %d: %f" % ((i), (whole_loss/200)))
      # if (i % 1) == 0:
      saver.save(sess, "my_net/Saliency" + str(int(i)) + ".ckpt")
